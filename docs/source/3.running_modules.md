# 3. Running Modules

## Overview

Each large brieflow process is referred to as a "module".
This includes preprocess, SBS, phenotype, merge, aggregate, and cluster.
Each module has its own config notebook (in brieflow-analysis) and targets/rules file (in brieflow).
A configuration notebook is used to configure a module's parameters, which are then used in the targets/rules.
The main `Snakefile` (at `brieflow/workflow/Snakefile`) connects each of these modules, as shown below:


<details>
<summary>View Snakefile code</summary>

```python
include: "targets/preprocess.smk"
include: "rules/preprocess.smk"

if "sbs" in config and len(sbs_wildcard_combos) > 0:

    # Include target and rule files
    include: "targets/sbs.smk"
    include: "rules/sbs.smk"

if "phenotype" in config and len(phenotype_wildcard_combos) > 0:

    # Include target and rule files
    include: "targets/phenotype.smk"
    include: "rules/phenotype.smk"

if "merge" in config:
    MERGE_COMBO_FP = Path(config["merge"]["merge_combo_fp"])
    merge_wildcard_combos = pd.read_csv(MERGE_COMBO_FP, sep="\t")

    # Include target and rule files
    include: "targets/merge.smk"
    include: "rules/merge.smk"

if "aggregate" in config:
    AGGREGATE_COMBO_FP = Path(config["aggregate"]["aggregate_combo_fp"])
    aggregate_wildcard_combos = pd.read_csv(AGGREGATE_COMBO_FP, sep="\t")

    # Include target and rule files
    include: "targets/aggregate.smk"
    include: "rules/aggregate.smk"

if "cluster" in config:
    CLUSTER_COMBO_FP = Path(config["cluster"]["cluster_combo_fp"])
    cluster_wildcard_combos = pd.read_csv(CLUSTER_COMBO_FP, sep="\t")

    # Include target and rule files
    include: "targets/cluster.smk"
    include: "rules/cluster.smk"
```

</details>

## Steps

A usual run of a module looks like:
1) Run the respective notebook in `brieflow-analysis/analysis` to determine parameters, which are then dumped into the config file at `brieflow-analysis/analysis/config/config.yml`.
Ex: run `brieflow-analysis/analysis/0.configure_preprocess_params.ipynb`.
2) Test the run with a dry run with a local run script.
Ex: run `1.run_preprocessing.sh` script with the `-n` snakemake modifier to ensure a dry run.
3) Complete a full run with a slurm run scirpt.
Ex: run `1.run_preprocessing_slurm.sh` script.

### OME-Zarr Output Format

Brieflow now supports **OME-Zarr (NGFF-compliant)** outputs alongside or instead of TIFFs. This cloud-optimized format enables efficient storage, streaming, and visualization of large imaging datasets.

#### Configuration

To enable OME-Zarr outputs, configure the following in your `config.yml`:

```yaml
preprocess:
  # Output format options
  output_formats:
    - tiff              # Enable TIFF outputs (optional)
    - zarr              # Enable OME-Zarr outputs
  downstream_input_format: zarr  # Use zarr for downstream modules (sbs, phenotype)
  
  # OME-Zarr chunk shape (Z, Y, X)
  omezarr_chunk_shape: [1, 1024, 1024]
  
  # Multiscale pyramid settings
  omezarr_coarsening_factor: 2    # Scale down by 2x at each level
  omezarr_max_levels: null        # Auto-determine levels (or set to fixed number)
  
  # OME-Zarr metadata configuration
  ome_zarr:
    output_root: brieflow_output/preprocess/ome_zarr
    chunk_xy: 1024                # Spatial chunk size
    scale_factors: [2, 4]         # Additional scale factors for visualization
    version: "0.4"                # NGFF version
    validate: true                # Enable compliance validation
  
  # Z-stack handling (default: max projection)
  sbs_preserve_z: false           # SBS: max project Z-stacks
  phenotype_preserve_z: false     # Phenotype: max project Z-stacks
```

#### Key Features

- **Dual Format Support**: Generate both TIFF and OME-Zarr outputs, or choose one
- **Downstream Format Selection**: Seamlessly use OME-Zarr in SBS and phenotype modules
- **NGFF Compliance**: Automatic validation against OME-NGFF v0.4 specification
- **Multiscale Pyramids**: Efficient visualization at multiple resolutions
- **Chunked Storage**: Optimized for parallel I/O and cloud access
- **Z-Stack Control**: Choose max projection or preserve full Z-stacks

#### Running with OME-Zarr

```bash
# Generate all preprocess outputs (TIFF and/or Zarr based on config)
snakemake --cores all --until all_preprocess

# Generate only OME-Zarr outputs
snakemake --cores all --until all_preprocess_ome_zarr

# Run full pipeline with Zarr inputs for downstream modules
snakemake --cores all
```

#### Compression Notes

**Note**: Compression is currently disabled in the pipeline due to `blosc` dependency resolution. The pipeline stores uncompressed chunks by default. Once the dependency is resolved, you'll be able to configure compression:

```yaml
# Future compression configuration (currently disabled)
# ome_zarr:
#   compression:
#     id: blosc
#     cname: zstd
#     clevel: 3
#     shuffle: 2
```

## Example Video

This video provides a step-by-step walkthrough of running a module in brieflow, including configuring parameters, testing with a dry run, and completing a full run using slurm scripts.
<iframe width="560" height="315" src="https://www.youtube.com/embed/0L5yYa1S8g0" frameborder="0" allowfullscreen title="Example Video: Running Modules"></iframe>

## Notes

- A slurm run's log files are output to `brieflow-analysis/analysis/slurm/slurm_output/main`
- The preprocessing, SBS, and phenotype modules have special slurm files to optimize running by grouping rule jobs and splitting snakemake runs by plate.
- Because of how snakemake generates DAGs, it is usually helpful to restrict the rules loaded for very large runs. When analyzing a large screen, we recommend commenting out the uncessary targets/rules in `brieflow/workflow/Snakefile`. Ex: when running aggregate, only the merge targets and aggregate rules/targets are neccesssary, so we can comment out the other components as shown below.
<details>
<summary>View restricted Snakefile example</summary>

```python
# include: "targets/preprocess.smk"
# include: "rules/preprocess.smk"

# if "sbs" in config and len(sbs_wildcard_combos) > 0:

#     # Include target and rule files
#     include: "targets/sbs.smk"
#     include: "rules/sbs.smk"

# if "phenotype" in config and len(phenotype_wildcard_combos) > 0:

#     # Include target and rule files
#     include: "targets/phenotype.smk"
#     include: "rules/phenotype.smk"

if "merge" in config:
    MERGE_COMBO_FP = Path(config["merge"]["merge_combo_fp"])
    merge_wildcard_combos = pd.read_csv(MERGE_COMBO_FP, sep="\t")

    # Include target and rule files
    include: "targets/merge.smk"
    # include: "rules/merge.smk"

if "aggregate" in config:
    AGGREGATE_COMBO_FP = Path(config["aggregate"]["aggregate_combo_fp"])
    aggregate_wildcard_combos = pd.read_csv(AGGREGATE_COMBO_FP, sep="\t")

    # Include target and rule files
    include: "targets/aggregate.smk"
    include: "rules/aggregate.smk"

# if "cluster" in config:
#     CLUSTER_COMBO_FP = Path(config["cluster"]["cluster_combo_fp"])
#     cluster_wildcard_combos = pd.read_csv(CLUSTER_COMBO_FP, sep="\t")

#     # Include target and rule files
#     include: "targets/cluster.smk"
#     include: "rules/cluster.smk"
```
</details>
^ Make sure to do a dry run to make sure the correct jobs will be run (step 2)! 
