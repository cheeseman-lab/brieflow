"""Aggregate Well Summaries Script for Snakemake.

Simple aggregation of individual well summary TSV files into plate-level summaries.
All input summaries are in wide format (one row per well).
"""

import pandas as pd
import numpy as np
from pathlib import Path
import re
import matplotlib.pyplot as plt

# Import the plotting function from your library
from lib.merge.eval_stitch import plot_cell_positions_plate_scatter

# Extract plate from params
plate = snakemake.params.plate
print(f"Aggregating summaries for plate {plate}")

# Helper function to extract well ID from path
def extract_well_info(file_path):
    """Extract plate and well from file path."""
    filename = Path(file_path).name
    match = re.search(r"P-(\d+)_W-([A-H]\d{1,2})", filename, re.IGNORECASE)
    if match:
        plate_id = match.group(1)
        well_id = match.group(2).upper()
        # Zero-pad well if needed (A3 -> A03)
        if len(well_id) == 2:
            well_id = well_id[0] + well_id[1:].zfill(2)
        return plate_id, well_id
    return None, None

# Process each summary type
summary_types = [
    ("alignment", snakemake.input.alignment_summary_paths, snakemake.output.alignment_summaries),
    ("merge", snakemake.input.merge_summary_paths, snakemake.output.cell_merge_summaries),
    ("dedup", snakemake.input.dedup_summary_paths, snakemake.output.dedup_summaries),
    ("sbs_matching", snakemake.input.sbs_matching_rates_paths, snakemake.output.sbs_matching_summaries),
    ("phenotype_matching", snakemake.input.phenotype_matching_rates_paths, snakemake.output.phenotype_matching_summaries)
]

for summary_type, input_paths, output_path in summary_types:
    print(f"\nProcessing {len(input_paths)} {summary_type} files...")
    
    dataframes = []
    
    for file_path in input_paths:
        try:
            # Load the file
            if Path(file_path).exists():
                df = pd.read_csv(file_path, sep="\t")
                
                if not df.empty:
                    # Extract and add plate/well info if missing
                    filename = Path(file_path).name
                    match = re.search(r"P-(\d+)_W-([A-H]\d{1,2})", filename, re.IGNORECASE)
                    if match:
                        plate_id = match.group(1)
                        well_id = match.group(2).upper()
                        # Zero-pad well if needed (A3 -> A03)
                        if len(well_id) == 2:
                            well_id = well_id[0] + well_id[1:].zfill(2)
                        df["plate"] = plate_id
                        df["well"] = well_id
                        dataframes.append(df)
                        print(f"  ‚úÖ {plate_id}-{well_id}")
                    else:
                        print(f"  ‚ö†Ô∏è  Could not extract well info from {file_path}")
                else:
                    print(f"  ‚ö†Ô∏è  Empty file: {file_path}")
            else:
                print(f"  ‚ö†Ô∏è  Missing file: {file_path}")
                
        except Exception as e:
            print(f"  ‚ùå Error loading {file_path}: {e}")
    
    # Combine and save
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        combined_df = combined_df.sort_values(["plate", "well"]).reset_index(drop=True)
        
        # Create output directory
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Save
        combined_df.to_csv(output_path, sep="\t", index=False)
        print(f"  üìÅ Saved {len(combined_df)} rows to {output_path}")
        
        # Quick success rate calculation
        if "status" in combined_df.columns:
            success_count = len(combined_df[combined_df["status"] != "failed"])
            print(f"  üìä Success rate: {success_count}/{len(combined_df)}")
        elif "error" in combined_df.columns:
            success_count = len(combined_df[combined_df["error"].isna() | (combined_df["error"] == "")])
            print(f"  üìä Success rate: {success_count}/{len(combined_df)}")
    else:
        # Create empty output file
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame(columns=["plate", "well"]).to_csv(output_path, sep="\t", index=False)
        print(f"  üìÅ Saved empty file to {output_path}")

print(f"\nGenerating cell position scatter plots...")

try:
    phenotype_files = snakemake.input.phenotype_cell_positions_paths
    sbs_files = snakemake.input.sbs_cell_positions_paths
    
    if phenotype_files:
        print(f"  Creating phenotype cell positions plot ({len(phenotype_files)} wells)...")
        fig, axes = plot_cell_positions_plate_scatter(
            parquet_files=phenotype_files,
            plate="6W",
            point_size=0.5,
            alpha=0.7,
            title=f"Plate {plate} - Phenotype Cell Positions",
            tile_column="tile",
            colorbar_label="Tile ID"
        )
        plt.savefig(snakemake.output.phenotype_cell_positions_plot, 
                   dpi=200, bbox_inches='tight')
        plt.close()
        print(f"    Saved: {snakemake.output.phenotype_cell_positions_plot}")
    
    if sbs_files:
        print(f"  Creating SBS cell positions plot ({len(sbs_files)} wells)...")
        fig, axes = plot_cell_positions_plate_scatter(
            parquet_files=sbs_files,
            plate="6W",
            point_size=0.5,
            alpha=0.7,
            title=f"Plate {plate} - SBS Cell Positions",
            tile_column="site",
            colorbar_label="Site ID"
        )
        plt.savefig(snakemake.output.sbs_cell_positions_plot, 
                   dpi=200, bbox_inches='tight')
        plt.close()
        print(f"    Saved: {snakemake.output.sbs_cell_positions_plot}")
    
    if not phenotype_files and not sbs_files:
        print(f"  Warning: No cell position files provided")
    else:
        plot_count = sum([bool(phenotype_files), bool(sbs_files)])
        print(f"  Generated {plot_count} scatter plot(s)")

except Exception as e:
    print(f"  Error generating scatter plots: {e}")

print(f"\nCompleted aggregation for plate {plate}")